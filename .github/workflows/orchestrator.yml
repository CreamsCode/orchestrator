name: Orchestrator Pipeline

on:
  workflow_dispatch: # Ejecuta el pipeline manualmente desde GitHub Actions

jobs:
  orchestrate:
    runs-on: ubuntu-latest

    env:
      COLLECTOR_REPO: "CreamsCode/collector"  # Repositorio del Collector
      DL_BUILDER_REPO: "CreamsCode/datalake-builder"    # Repositorio del Listener
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_SESSION_TOKEN: ${{ secrets.AWS_SESSION_TOKEN }}

    steps:
      - name: Set up GitHub CLI
        run: |
          sudo apt-get update
          sudo apt-get install -y gh
          gh auth login --with-token <<< "${{ secrets.MY_GITHUB_TOKEN }}"

      - name: Trigger Collector Pipeline
        run: |
          gh workflow run collector.yml --repo $COLLECTOR_REPO \
            -f aws_access_key_id="${{ secrets.AWS_ACCESS_KEY_ID }}" \
            -f aws_secret_access_key="${{ secrets.AWS_SECRET_ACCESS_KEY }}" \
            -f aws_session_token="${{ secrets.AWS_SESSION_TOKEN }}"

      - name: Wait for Collector Completion
        run: |
          echo "Waiting for Collector pipeline to complete..."
          sleep 75
        

      - name: Get Outputs from Parameter Store
        run: |
          SQS_QUEUE_URL=$(aws ssm get-parameter --name "sqs_queue_url" --query "Parameter.Value" --output text)
          SCRAPER_IP=$(aws ssm get-parameter --name "scraper_ip" --query "Parameter.Value" --output text)

          echo "SQS_QUEUE_URL=$SQS_QUEUE_URL"
          echo "SCRAPER_IP=$SCRAPER_IP"

          echo "SQS_QUEUE_URL=$SQS_QUEUE_URL" >> $GITHUB_ENV
          echo "SCRAPER_IP=$SCRAPER_IP" >> $GITHUB_ENV
      
      

      - name: Trigger Listener Pipeline
        run: |
          gh workflow run listener.yml --repo $DL_BUILDER_REPO \
            -f sqs_queue_url="${{ env.SQS_QUEUE_URL }}" \
            -f scraper_ip="${{ env.SCRAPER_IP }}"
